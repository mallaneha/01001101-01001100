{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "negative-abraham",
   "metadata": {},
   "source": [
    "# Clustering – Finding Related Posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-macedonia",
   "metadata": {},
   "source": [
    "## Preprocessing – similarity measured as a similar number of common words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-belize",
   "metadata": {},
   "source": [
    "### 1. Converting raw text into a bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "encouraging-welsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earlier-course",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "X = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "purple-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-audio",
   "metadata": {},
   "source": [
    "### 2. Counting words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affecting-safety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "posts = [open(os.path.join(r'C:\\Users\\neha\\Documents\\ML-PA\\ML-pa3\\data', f)).read() for f in os.listdir(r'C:\\Users\\neha\\Documents\\ML-PA\\ML-pa3\\data')]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stable-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'can', 'contains', 'databases', 'get', 'huge', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'save', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stock-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "basic-factory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "signed-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fifty-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much \n",
      "interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases can get huge.\n",
      "=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=1.41: Imaging databases store images.\n",
      "=== Post 4 with dist=5.10: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    if posts[i] == new_post:\n",
    "        continue   \n",
    "    post_vec = X_train.getrow(i)    \n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, posts[i]))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "preliminary-brand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fluid-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 3 0 0 3 3 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-aerospace",
   "metadata": {},
   "source": [
    "### 3. Normalizing word count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bulgarian-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "republican-burst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much \n",
      "interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases can get huge.\n",
      "=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store images.\n",
      "=== Post 4 with dist=0.77: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    if posts[i] == new_post:\n",
    "        continue   \n",
    "    post_vec = X_train.getrow(i)    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, posts[i]))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-democracy",
   "metadata": {},
   "source": [
    "### 4. Removing less important words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "micro-virginia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 15\n",
      "['actually', 'contains', 'databases', 'huge', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'save', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "sorted(vectorizer.get_stop_words())[0:20]\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples,  num_features))\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intelligent-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much \n",
      "interesting stuff.\n",
      "=== Post 1 with dist=0.61: Imaging databases can get huge.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store images.\n",
      "=== Post 4 with dist=0.77: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
      "Best post is 1 with dist=0.61\n"
     ]
    }
   ],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    if posts[i] == new_post:\n",
    "        continue   \n",
    "    post_vec = X_train.getrow(i)    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, posts[i]))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-costa",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-subscriber",
   "metadata": {},
   "source": [
    "### Installing and using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "polar-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sound-nation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "elder-assistant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imaging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "prime-little",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "emotional-calculation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imagination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "valued-relationship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imagine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lasting-kernel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "disturbed-michigan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "married-hormone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"bought\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-force",
   "metadata": {},
   "source": [
    "### Extending the vectorizer with NLTK's stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "precise-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "alike-recycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much \n",
      "interesting stuff.\n",
      "=== Post 1 with dist=0.61: Imaging databases can get huge.\n",
      "=== Post 2 with dist=0.63: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=0.52: Imaging databases store images.\n",
      "=== Post 4 with dist=0.52: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
      "Best post is 3 with dist=0.52\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    if posts[i] == new_post:\n",
    "        continue   \n",
    "    post_vec = X_train.getrow(i)    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, posts[i]))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-india",
   "metadata": {},
   "source": [
    "### Stop words on steroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "progressive-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def tfidf(term, doc, corpus):\n",
    "    tf = doc.count(term) / len(doc)\n",
    "    num_docs_with_term = len([d for d in corpus if term in d])\n",
    "    idf = np.log(len(corpus) / num_docs_with_term)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accurate-riverside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "print(tfidf(\"a\", a, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "consolidated-france",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "short-postcard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "unlike-birmingham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27031007207210955\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "turkish-deployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "injured-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13515503603605478\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "qualified-generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3662040962227032\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "direct-petroleum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much \n",
      "interesting stuff.\n",
      "=== Post 1 with dist=0.87: Imaging databases can get huge.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "=== Post 3 with dist=0.63: Imaging databases store images.\n",
      "=== Post 4 with dist=0.63: Imaging databases store images. Imaging databases store images. Imaging databases store images.\n",
      "Best post is 3 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english', decode_error='ignore')\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    if posts[i] == new_post:\n",
    "        continue   \n",
    "    post_vec = X_train.getrow(i)    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, posts[i]))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-saudi",
   "metadata": {},
   "source": [
    "# Clustering: K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "freelance-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "all_data = sklearn.datasets.fetch_20newsgroups(subset='all')\n",
    "print(len(all_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "resistant-victim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(all_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mature-factory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train')\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "continent-partition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test')\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "guilty-familiar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n"
     ]
    }
   ],
   "source": [
    "groups = ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n",
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train', categories=groups)\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "false-potential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test', categories=groups)\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-issue",
   "metadata": {},
   "source": [
    "## Clustering posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dutch-adaptation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 3529, #features: 4712\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5, stop_words='english', decode_error='ignore')\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "loved-absorption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 5899.5595831471655\n",
      "Iteration 1, inertia 3218.297747726279\n",
      "Iteration 2, inertia 3184.3328334733214\n",
      "Iteration 3, inertia 3164.867358130041\n",
      "Iteration 4, inertia 3152.003949571175\n",
      "Iteration 5, inertia 3143.1109963529184\n",
      "Iteration 6, inertia 3136.2559774422048\n",
      "Iteration 7, inertia 3129.3248717684405\n",
      "Iteration 8, inertia 3124.5674798201394\n",
      "Iteration 9, inertia 3121.9001105797406\n",
      "Iteration 10, inertia 3120.209894571872\n",
      "Iteration 11, inertia 3118.62745619288\n",
      "Iteration 12, inertia 3117.362525978361\n",
      "Iteration 13, inertia 3116.8112664390364\n",
      "Iteration 14, inertia 3116.587892365764\n",
      "Iteration 15, inertia 3116.417048753848\n",
      "Iteration 16, inertia 3115.760414808626\n",
      "Iteration 17, inertia 3115.3736535034473\n",
      "Iteration 18, inertia 3115.155454436256\n",
      "Iteration 19, inertia 3114.9491175607545\n",
      "Iteration 20, inertia 3114.5149932662175\n",
      "Iteration 21, inertia 3113.9369169464094\n",
      "Iteration 22, inertia 3113.719999300366\n",
      "Iteration 23, inertia 3113.547519005385\n",
      "Iteration 24, inertia 3113.474905847476\n",
      "Iteration 25, inertia 3113.4467573371267\n",
      "Converged at iteration 25: strict convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(init='random', n_clusters=50, n_init=1, random_state=3, verbose=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 50\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=num_clusters, init='random', n_init=1, verbose=1, random_state=3)\n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "split-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 17 47 ... 41 14 16]\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sized-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-buffer",
   "metadata": {},
   "source": [
    "## Solving our initial challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "marked-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = \"Disk drive problems. Hi, I have a problem with my hard disk. After 1 year it is working only sporadically now. I tried to format it, but now it doesn't boot any more. Any ideas? Thanks.\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "accomplished-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_indices = (km.labels_==new_post_label).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aggregate-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, all_data.data[i]))\n",
    "similar = sorted(similar)\n",
    "print(len(similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "temporal-thanks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0378441731334074, 'From: gtd597a@prism.gatech.EDU (Hrivnak)\\nSubject: Goalie mask poll update 4/22/93\\nSummary: *** KEEP SENDING IN THOSE VOTES!!! ***\\nOrganization: Georgia Institute of Technology\\nLines: 38\\n\\n\\n\\tCurtis Joseph and Ray LeBlanc have made some big moves in the\\npoll recently. Hextall has shown some strong movement as well. Kirk\\nMcLean and Tom Barrasso (I can\\'t see why) have been added to the list\\nrecently. Keep sending in those votes.\\n\\nCurrent votes for favorite goalie masks (3pts - 1st, 2pts - 2nd, 1pt - 3rd)\\n\\nPlayer                    Team                 Pts       Votes\\n--------------------------------------------------------------\\n1. Ed Belfour             Chicago              32         15\\n   Curtis Joseph          St. Louis            32         13 \\n3. Andy Moog              Boston               30         13\\n4. Brian Hayward          San Jose             26         10 \\n5. Ron Hextall            Quebec               16          8\\n6. Grant Fuhr             Buffalo              12          5 \\n7. Gerry Cheevers         Boston (retired)     11          6 \\n8. John Vanbeisbrouck     NY Rangers           10          4\\n9. Ray LeBlanc            USA Olympic           7          3\\n10. Mike Richter          NY Rangers            6          3\\n11. Manon Rheaume         Atlanta (IHL)         5          2\\n12. Don Beaupre           Washington            4          2\\n    Ken Dryden            Montreal (retired)    4          2\\n--------------------------------------------------------------\\nOthers receiving less than 4pts: Mike Vernon (Cal), Clint\\n Malarchuk (Buf/SD,IHL), Tommy Soderstrom (Phil), Tom Barrasso (Pit),\\n Artus Irbe (SJ), Tim Cheveldae (Det), Sean Burke (NJ),\\n Rick Wamsley (Tor,ret), Jon Casey (Minn), Bob Essensa (Win),\\n Glenn Healy (NYI), Tony Espo (Chi), Gilles Gratton (Bos),\\n Rod Stauber (LA), Gump Worsley (Mtl/NYR), Pat Jablonski (TB),\\n Grant Fuhr (Tor), Felix Potvin (Tor), Stephane Beauregard (Win),\\n Mark Fitzpatrick (NYI), Chico Resch (NYI), Kirk McLean (Van)\\n\\n-- \\nGO SKINS!    ||\"Now for the next question... Does emotional music have quite\\nGO BRAVES!   ||   an effect on you?\" - Mike Patton, Faith No More \\nGO HORNETS!  ||\\nGO CAPITALS! ||Mike Friedman (Hrivnak fan!) Internet: gtd597a@prism.gatech.edu\\n') \n",
      " \n",
      "\n",
      "(1.1716497460538475, \"From: anovak@twain.ucs.umass.edu (Tree Hugger)\\nSubject: Rush Limbaugh's address (oops from before)\\nOrganization: /usr/users/user3/anovak/.organization\\nLines: 30\\nNNTP-Posting-Host: twain.ucs.umass.edu\\n\\n\\tI think this didn't get posted before (I've been reading\\nUSENET for the longest time, but never had much interest in posting\\nuntil recently).  This is what I typed before:\\n\\n\\tI have written Mr. Limbaugh before, and I loathe to use the\\nname Rush in association with him, because he is unworthy to have a\\nname in common with some of the greatest musicians in our time, the\\nBAND, Rush.  \\n\\tHis address, as some of you wanted is:\\n\\t70277.2502@compuserve.com\\n\\tHe has been to wrapped up in himself to respond to me, but\\nmaybe some of you will have better luck.  :)  bye!\\n--\\n\\tg'bye for now...\\n\\n\\t-=I   Tree   I=-                  a.k.a. Andy Novak\\n\\n---------------------------------------------------------------------\\n            anovak@titan.ucs.umass.edu \\n                  anovak@twain.ucs.umass.edu\\n---------------------------------------------------------------------\\n--\\n\\tg'bye for now...\\n\\n\\t-=I   Tree   I=-                  a.k.a. Andy Novak\\n\\n---------------------------------------------------------------------\\n            anovak@titan.ucs.umass.edu \\n                  anovak@twain.ucs.umass.edu\\n--------------------------------------------------------------------- \\n\") \n",
      " \n",
      "\n",
      "(1.3118266609870632, 'From: klinker@itd.nrl.navy.mil (Eric Klinker)\\nSubject: Re: best homeruns\\nOrganization: Naval Research Lab, Washington, DC\\nDistribution: rec\\nLines: 7\\n\\nThe best one I saw last year was Willie McGee off Matthews (I think?) in\\nPhillie.  A fierce line drive that was still rising when it hit thE\\nsecond deck facade at the Vet.  Willie McGee had one homerun last year.\\n\\n\\n-- \\n                                   Eric\\n')\n"
     ]
    }
   ],
   "source": [
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[int(len(similar)/10)]\n",
    "show_at_3 = similar[int(len(similar)/2)]\n",
    "\n",
    "print(show_at_1, \"\\n \\n\")\n",
    "print(show_at_2, \"\\n \\n\")\n",
    "print(show_at_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-confirmation",
   "metadata": {},
   "source": [
    "## Another look at noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "comparative-effect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization: The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\\n\\n==============================================================================\\n', 'comp.graphics')\n"
     ]
    }
   ],
   "source": [
    "post_group = zip(train_data.data, train_data.target)\n",
    "all = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]\n",
    "graphics = sorted([post for post in all if post[2]=='comp.graphics'])\n",
    "print(graphics[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "reverse-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test', 'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line', 'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']\n"
     ]
    }
   ],
   "source": [
    "noise_post = graphics[5][1]\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "print(list(analyzer(noise_post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "handy-trauma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']\n"
     ]
    }
   ],
   "source": [
    "useful = set(analyzer(noise_post)).intersection(vectorizer.get_feature_names())\n",
    "print(sorted(useful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "viral-hurricane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF(ac)=3.51\n",
      "IDF(birmingham)=6.77\n",
      "IDF(host)=1.74\n",
      "IDF(kingdom)=6.68\n",
      "IDF(nntp)=1.77\n",
      "IDF(sorri)=4.14\n",
      "IDF(test)=3.83\n",
      "IDF(uk)=3.70\n",
      "IDF(unit)=4.42\n",
      "IDF(univers)=1.91\n"
     ]
    }
   ],
   "source": [
    "for term in sorted(useful):\n",
    "    print('IDF(%s)=%.2f'%(term, vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
